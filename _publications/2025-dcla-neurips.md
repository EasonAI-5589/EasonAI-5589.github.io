---
title: "Mitigating Hallucinations via Inter-Layer Consistency Aggregation in Large Vision-Language Models"
authors: "K. Tang*, J. You*, X. Ge, H. Li, Yichen Guo, X. Huang"
venue: "NeurIPS 2025"
year: 2025
type: "conference"
status: "under_review"
pdf: "/files/dcla-neurips2025.pdf"
code: "https://github.com/EasonAI-5589/DCLA"
note: "*Equal contribution"
---

We propose DCLA, a novel training-free decoding framework to reduce hallucinations in Large Vision-Language Models (LVLMs) via inter-layer consistency aggregation. DCLA introduces a novel training-free decoding mechanism that improves semantic consistency during inference by constructing dynamic semantic references through aggregating hidden representations and correcting deviated layers. The method achieved superior performance compared to existing decoding methods across hallucination benchmarks, demonstrating strong generalization capabilities.
